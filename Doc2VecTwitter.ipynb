{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Modules-Definition\" data-toc-modified-id=\"Modules-Definition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Modules Definition</a></span></li><li><span><a href=\"#Read-and-trim-dictionary\" data-toc-modified-id=\"Read-and-trim-dictionary-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Read and trim dictionary</a></span></li><li><span><a href=\"#Import-and-Preprocess-Tweets\" data-toc-modified-id=\"Import-and-Preprocess-Tweets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Import and Preprocess Tweets</a></span></li><li><span><a href=\"#Create-and-save-doc2vec-model\" data-toc-modified-id=\"Create-and-save-doc2vec-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create and save doc2vec model</a></span></li><li><span><a href=\"#Train-the-Model:-The-Training-Dataset\" data-toc-modified-id=\"Train-the-Model:-The-Training-Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Train the Model: The Training Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-neighbors-classifier\" data-toc-modified-id=\"K-neighbors-classifier-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>K-neighbors-classifier</a></span></li><li><span><a href=\"#Random-Forest-and/or-Support-Vector-Machine\" data-toc-modified-id=\"Random-Forest-and/or-Support-Vector-Machine-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Random Forest and/or Support Vector Machine</a></span></li></ul></li><li><span><a href=\"#General-Classification-Model\" data-toc-modified-id=\"General-Classification-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>General Classification Model</a></span></li><li><span><a href=\"#Classification-of-unseen-data\" data-toc-modified-id=\"Classification-of-unseen-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Classification of unseen data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-Network\" data-toc-modified-id=\"Neural-Network-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Neural Network</a></span></li></ul></li><li><span><a href=\"#Automatic-Classification\" data-toc-modified-id=\"Automatic-Classification-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Automatic Classification</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/marco/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import doc2vec\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import namedtuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modules Definition\n",
    "\n",
    "Define the following modules:\n",
    "- preProcess(): Text preprocessing, to tokenize, remove stopwords, and sentences that are too short.\n",
    "- addTags(): This is needed to change the structure of each sentence (tweet) in the format required by doc2vec. We need to add a tag to each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preProcess(df):\n",
    "    docs = []\n",
    "    corpus = []\n",
    "    idOriginal = []\n",
    "    countWords = 0\n",
    "    countDict = 0\n",
    "    countShort = 0\n",
    "    i = 0\n",
    "    for ss in df[\"documents\"]:\n",
    "        if (i % 1000 == 0):\n",
    "            print(\"Doc {0:5d} has been processed\".format(i))\n",
    "        i += 1\n",
    "        tokens = word_tokenize(ss)\n",
    "        #print(tokens)\n",
    "        countWords += len(tokens)\n",
    "        for w in tokens:\n",
    "            #print(\"w \", w, \" in dictionary? \", w in dct.token2id)\n",
    "            if w not in dct.token2id:\n",
    "                countDict += 1\n",
    "        words = [w.lower() for w in word_tokenize(ss) if w not in stop_words and w in dct.token2id]\n",
    "        if len(words) > 2:\n",
    "            docs.append(words)\n",
    "            corpus.append(ss)\n",
    "            idOriginal.append(df.iloc[i-1][0])\n",
    "        else:\n",
    "            countShort += 1\n",
    "    print(\"Dict = \", countDict, \" (\", countWords, \" - \", countDict/countWords, \" ) and Short = \", countShort)\n",
    "    \n",
    "    return docs, corpus, idOriginal\n",
    "\n",
    "def addTags(docs):\n",
    "    dTags = []\n",
    "    analyzedDocument = namedtuple(\"AnalyzedDocument\", \"words tags\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        tags = [i]\n",
    "        dTags.append(analyzedDocument(doc, tags))\n",
    "    return dTags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read and trim dictionary \n",
    "\n",
    "We import a dictionary built on the Italian wikipedia and we cut it to a maximum number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inp = \"../wiki/Wikipedia_Word2vec-master/v1/itwiki-20180520-pages-articles.xml.bz2\"\n",
    "wiki = WikiCorpus(inp,lemmatize=False, dictionary={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dct = Dictionary.load_from_text(\"/home/marco/gdrive/research/nlp/wiki/dictWiki.txt\")\n",
    "print(\"Length dictionary before filter = \", len(dct))\n",
    "dct.filter_extremes(keep_n=500000)\n",
    "print(\"Length dictionary after filter = \", len(dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"italian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import and Preprocess Tweets\n",
    "\n",
    "We read the __full list__ of tweets (over 23k tweets), to build a doc2vec model with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(dTagsAll)):\n",
    "    if (dTagsAll[i][1][0] != idOriginal[i]-1):\n",
    "        print(dTagsAll[i][1][0], \" vs \", idOriginal[i]-1)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/listTweets.csv\")\n",
    "df.head()\n",
    "docs, corpus, idOriginal = preProcess(df)\n",
    "dTagsAll = addTags(docs)\n",
    "print(\"Total nr. of documents = \", len(docs))\n",
    "#for i in range(len(docs)):\n",
    "#    print(dTagsAll[i][1], \" :: \", dTagsAll[i][0])\n",
    "hashtags = df[df[\"ID\"].isin(idOriginal)][\"HASHTAG\"]\n",
    "hashtags = [w.lower() for w in hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfProcessed = pd.DataFrame({\n",
    "    \"idOriginal\": idOriginal,\n",
    "    \"idCurrent\" : np.arange(len(docs)),\n",
    "    \"docs\"      : docs,\n",
    "    \"corpus\"    : corpus,\n",
    "    \"hashtag\"   : hashtags\n",
    "})\n",
    "dfProcessed.head()\n",
    "dfProcessed.to_csv(\"data/preProcessedAll.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create and save doc2vec model\n",
    "\n",
    "Note that there are two implementations of the doc2vec model:\n",
    "\n",
    "- doc2vecTwitter.model: This is the model obtained when the option \"dm=0\" is used in the generation of the model using the gensim module.\n",
    "- doc2vecTwitter.model.dm: This model is obtained activating \"dm=1\". Some papers in the literature say that the doc2vec model obtained using \"dm=1\" is of higher quality than the one obtained with \"dm=0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec(size=300, window=10, min_count=2, iter=100, workers=4, dm=0, max_vocab_size=10000)\n",
    "model.build_vocab(dTagsAll)\n",
    "model.train(dTagsAll, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.init_sims(replace=True)\n",
    "model.save(\"doc2vecTwitter.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "modelDM = doc2vec.Doc2Vec(size=300, window=10, min_count=2, iter=100, workers=4, dm=1, max_vocab_size=10000)\n",
    "modelDM.build_vocab(dTagsAll)\n",
    "modelDM.train(dTagsAll, total_examples=modelDM.corpus_count, epochs=modelDM.iter)\n",
    "modelDM.init_sims(replace=True)\n",
    "modelDM.save(\"doc2vecTwitter.model.dm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load(\"doc2vecTwitter.model\") # load one of the two versions of doc2vec model\n",
    "# this is a little test to assess the quality of the doc2vec model...\n",
    "withTest = 0\n",
    "if withTest:\n",
    "    x = []\n",
    "    for idx in range(1000):\n",
    "        v0 = model.infer_vector(dTagsAll[idx].words, steps=10000)\n",
    "        sims = model.docvecs.most_similar([v0], topn=len(docs))\n",
    "        rank = [idx for idx,sim in sims].index(idx)\n",
    "        x.append(rank)\n",
    "        if idx % 50 == 0:\n",
    "            print(\"Vector \", idx, \" has rank \", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(x))\n",
    "print(np.median(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"banca\", topn=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train the Model: The Training Dataset\n",
    "\n",
    "We now import the training dataset, i.e., the one in which tweets have been manually labeled. The current version contains only labels $0$ and $1$. Therefore, we can use standard binary classification algorithms. Currently, we are trying the following algorithms:\n",
    "- k-neighbors-classifier\n",
    "- random forest\n",
    "- support vector machine\n",
    "We also do some calibration with Support Vector machine, to determine an appropriate choice of $\\gamma$ and $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dTrain1000 = pd.read_csv(\"data/sample_1000.csv\")\n",
    "dTrain1000.head()\n",
    "docsTraining, corpusTraining, idTraining = preProcess(dTrain1000)\n",
    "dTags1000 = addTags(docsTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get embedding for Hashtag (inferred)\n",
    "hashtags = dTrain1000[dTrain1000[\"ID\"].isin(idTraining)][\"HASHTAG\"]\n",
    "vecHashtags = []\n",
    "print(\"banca\" in model.wv)\n",
    "count = 0\n",
    "for ht in hashtags:\n",
    "    vecHashtags.append(model.infer_vector(ht))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(dTrain1000.iloc[0,1])\n",
    "print(dTrain1000.iloc[0,0])\n",
    "#for i in range(10):\n",
    "#    print(\"i = \", i , \" vs tag \", dTags1000[i][1], \" :: \", dTags1000[i].words)\n",
    "# get mapping from original id to TAG value in doc2vec\n",
    "id2tag = {orig:curr for orig,curr in zip(dfProcessed.idOriginal, dfProcessed.idCurrent)}\n",
    "print(len(dTrain1000), len(docsTraining))\n",
    "print(id2tag[64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# infer the vector embeddings using doc2vec\n",
    "# NOTE: Rem that some tweets have been eliminated from dTrain1000\n",
    "Y = dTrain1000[dTrain1000[\"ID\"].isin(idTraining)][\"COD_SE\"]\n",
    "tags = [id2tag[i] for i in idTraining]\n",
    "\n",
    "y = [y for y in Y]\n",
    "X = []\n",
    "#nn = 100\n",
    "for idx in range(len(idTraining)):\n",
    "#for idx in range(100):\n",
    "    tag = tags[idx]\n",
    "    if idx % 50 == 0:\n",
    "        print(\"Vector \", idx, \" with TAG = \", tag)\n",
    "    #X.append(model.infer_vector(dTags[idx].words, steps=10000))\n",
    "    if docsTraining[idx] != dTagsAll[tag].words:\n",
    "        print(\"D2V(\", tag, \") \", dTagsAll[tag].words)\n",
    "        print(\"docs = \", dTrain1000.iloc[idx][1])\n",
    "        print(\"doc =  \", docsTraining[idx])\n",
    "        input(\"aka\")\n",
    "    xi = [i for i in model.docvecs[tag]]\n",
    "    #print(xi)\n",
    "    for i in vecHashtags[idx]:\n",
    "        xi.append(i)\n",
    "    X.append(xi)\n",
    "print(\"D2V(\", tag, \") \", dTagsAll[tag].words)\n",
    "print(\"docs = \", dTrain1000.iloc[idx][1])\n",
    "print(\"doc =  \", docsTraining[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## K-neighbors-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=2)  \n",
    "classifier.fit(X, y)  \n",
    "y_pred = classifier.predict(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y, y_pred))  \n",
    "print(classification_report(y, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ypd = pd.DataFrame({\n",
    "    \"id\": idOriginal,\n",
    "    \"corpus\": corpusTraining,\n",
    "    \"y\":y,\n",
    "    \"yPred\":y_pred})\n",
    "ypd.to_csv(\"data/yPred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random Forest and/or Support Vector Machine\n",
    "\n",
    "Here we use a k-fold cross-validation method, to determine a more \"objective\" measure of the accuracy of the method. Note that such k-fold validation can easily be activated for random forest as well (just uncomment the corresponding lines and comment out those belonging to SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** ** ** Using Method  NN  ** ** **\n",
      "Nr. of Folds =  5\n",
      "Fold  1\n",
      "[[188   0]\n",
      " [  0  11]]\n",
      "Fold  2\n",
      "[[187   1]\n",
      " [  0  11]]\n",
      "Fold  3\n",
      "[[191   2]\n",
      " [  0   6]]\n",
      "Fold  4\n",
      "[[194   0]\n",
      " [  0   4]]\n",
      "Fold  5\n",
      "[[182   4]\n",
      " [  0  12]]\n",
      "AVGS ::\n",
      "Correct Positives =  1.0\n",
      "False Positives   =  0.116666666667\n",
      "Accuracy          =  0.992944520583\n",
      "Score             =  0.992944520583\n"
     ]
    }
   ],
   "source": [
    "method = \"NN\" # change it to \"RF\" or \"SVM\"\n",
    "print(\"** ** ** Using Method \", method, \" ** ** **\")\n",
    "KK = 5\n",
    "kf = KFold(n_splits=KK, shuffle=True) # Define the split - into 5 folds \n",
    "print(\"Nr. of Folds = \", kf.get_n_splits(X)) # returns the number of splitting iterations in the cross-validator\n",
    "i = 0\n",
    "param_grid = {'C': [1, 5, 10, 50,100,1000,10000,100000,1000000],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]}\n",
    "avgPos      = 0.0\n",
    "avgFalsePos = 0.0\n",
    "avgAccuracy = 0.0\n",
    "avgScore    = 0.0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TEST:\", test_index)\n",
    "    i += 1\n",
    "    print(\"Fold \", i)\n",
    "    X_train, X_test = np.array(X)[train_index],  np.array(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index],  np.array(y)[test_index]\n",
    "    if method == \"RF\":\n",
    "        mm = RandomForestClassifier(n_estimators=500, criterion=\"gini\", n_jobs=-1, max_features=\"sqrt\",class_weight={0:1, 1:100000})\n",
    "        mm.fit(X_train, np.array(y_train))\n",
    "    elif method == \"SVM\":\n",
    "        #mm = SVC(kernel='rbf', class_weight={0: 1, 1: 100000}, gamma=0.0001, C=1E10)\n",
    "        mm = SVC(kernel='sigmoid', class_weight={0: 1, 1: 100000}, gamma=0.001, C=1E10)\n",
    "        # this is used to run a grid search on model parameters\n",
    "        #grid = GridSearchCV(modelSVM, param_grid)\n",
    "        #grid.fit(X_train, y_train)\n",
    "        #print(\"Best Parameters = \", grid.best_params_)    \n",
    "        mm.fit(X_train, np.array(y_train))\n",
    "    elif method == \"NN\":\n",
    "        mm = MLPClassifier(solver='lbfgs', alpha=0.5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "        mm.fit(X, y)    \n",
    " \n",
    "    # relevant output\n",
    "    #training_predictions = modelRF.predict(X_train)\n",
    "    testing_predictions = mm.predict(X_test)\n",
    "    #print(confusion_matrix(y_train, training_predictions))\n",
    "    #print(classification_report(y_train, training_predictions)) \n",
    "    tt = confusion_matrix(y_test, testing_predictions)\n",
    "    print(tt)\n",
    "    #print(\"Percent Correct Positive = \", tt[1][1]/(tt[1][0] + tt[1][1]))\n",
    "    #print(\"Percent False Positive = \", tt[0][1]/(tt[0][1] + tt[1][1]))\n",
    "    avgPos += tt[1][1]/(tt[1][0] + tt[1][1])\n",
    "    avgFalsePos += tt[0][1]/(tt[0][1] + tt[1][1])\n",
    "    avgAccuracy += (tt[0][0] + tt[1][1])/sum(sum(tt))\n",
    "    avgScore += mm.score(X_test, y_test) # same as accuracy\n",
    "    #print(classification_report(y_test, testing_predictions)) \n",
    "\n",
    "print(\"AVGS ::\")\n",
    "print(\"Correct Positives = \", avgPos/KK)\n",
    "print(\"False Positives   = \", avgFalsePos/KK)\n",
    "print(\"Accuracy          = \", avgAccuracy/KK)\n",
    "print(\"Score             = \", avgScore/KK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# General Classification Model\n",
    "\n",
    "This is the model we want to use on the unseen data. The choice of method and paramenters depend on the calibration phase described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create general model to be used on unseen data\n",
    "def createModel(method, kk, X, y, dm):\n",
    "    if method == \"RF\":\n",
    "        print(\"** ** ** Using Method \", method, \" with dm = \", dm, \" ** ** **\")\n",
    "        mmAll = RandomForestClassifier(n_estimators=500, criterion=\"gini\", n_jobs=-1, max_features=\"sqrt\",class_weight={0:1, 1:100000})\n",
    "    elif method == \"SVM\":\n",
    "        print(\"** ** ** Using Method \", method, \" with dm = \", dm, \" [ kernel = \", kk, \"] ** ** **\")\n",
    "        mmAll = SVC(kernel=kk, class_weight={0: 1, 1: 100000}, gamma=0.0001, C=1E10)\n",
    "        #mmAll = SVC(kernel='linear', class_weight={0: 0.001, 1: 10000000}, gamma=0.01, C=1)\n",
    "    elif method == \"NN\":\n",
    "        print(\"** ** ** Using Method \", method, \" with dm = \", dm, \" ** ** **\")\n",
    "        mmAll = MLPClassifier(solver='lbfgs', alpha=0.5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "    mmAll.fit(X, np.array(y))\n",
    "    training_predictions = mmAll.predict(X)\n",
    "    #print(confusion_matrix(y, training_predictions))\n",
    "    #print(classification_report(y, training_predictions)) \n",
    "    \n",
    "    return mmAll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classification of unseen data\n",
    "\n",
    "Read the full list of tweets (over 23k) and apply the best classifier (learned above) to this data. The resulting classification is stored on a dataframe and written on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this is no longer needed\n",
    "#dfAll = pd.read_csv(\"data/listTweets.csv\")\n",
    "#docsAll, corpusAll, idOriginalAll = preProcess(dfAll)\n",
    "#dTagsAll = addTags(docsAll)\n",
    "#print(\"Tot documents: \", len(docsAll))\n",
    "#print(dTagsAll[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load the model and infer the embedding vectors\n",
    "model = doc2vec.Doc2Vec.load(\"doc2vecTwitter.model.dm\")\n",
    "if os.path.exists(\"data/XAll.csv.dm\") or os.path.exists(\"data/XAll.csv\"):\n",
    "    print(\"SKIPPING tweets embedding infer vector... Load tweets embedding from disk \")\n",
    "    XAll = []\n",
    "    with open(\"data/XAll.csv.dm\",\"r\") as f:\n",
    "        rr = csv.reader(f)\n",
    "        for row in rr:\n",
    "            XAll.append(row)\n",
    "    print(\"Read \", len(XAll), \" vector embeddings.\")\n",
    "else:\n",
    "    print(\"Computing tweets embedding...\")\n",
    "    XAll = []\n",
    "    for idx in range(len(dTagsAll)):\n",
    "        if idx % 25 == 0:\n",
    "            print(\"Vector \", idx)\n",
    "        XAll.append(model.infer_vector(dTagsAll[idx].words, steps=10000))\n",
    "    # write in csv format the embeddings. This should save some time\n",
    "    with open(\"data/XAll.csv.dm\",\"w\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerows(XAll)\n",
    "        \n",
    "print(\"Step completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X, y)       \n",
    "training_predictions = clf.predict(X)\n",
    "print(confusion_matrix(y, training_predictions))\n",
    "print(classification_report(y, training_predictions)) \n",
    "print(clf.score(X,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Automatic Classification\n",
    "\n",
    "We first define and load needed libraries. Next, we run the cycle over different method (`dm=0` and `dm=1`), different classification algorithms (`RF` and `SVM`), and in the case of Support Vector Machine, different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preProcess(df):\n",
    "    docs = []\n",
    "    corpus = []\n",
    "    idOriginal = []\n",
    "    countWords = 0\n",
    "    countDict = 0\n",
    "    countShort = 0\n",
    "    i = 0\n",
    "    print(\"Preprocessing Full Set of Documents ... \")\n",
    "    for ss in df[\"documents\"]:\n",
    "        if (i % 10000 == 0):\n",
    "            print(\"Doc {0:5d} has been processed\".format(i))\n",
    "        i += 1\n",
    "        tokens = word_tokenize(ss)\n",
    "        #print(tokens)\n",
    "        countWords += len(tokens)\n",
    "        for w in tokens:\n",
    "            #print(\"w \", w, \" in dictionary? \", w in dct.token2id)\n",
    "            if w not in dct.token2id:\n",
    "                countDict += 1\n",
    "        words = [w.lower() for w in word_tokenize(ss) if w not in stop_words and w in dct.token2id]\n",
    "        if len(words) > 2:\n",
    "            docs.append(words)\n",
    "            corpus.append(ss)\n",
    "            idOriginal.append(df.iloc[i-1][0])\n",
    "        else:\n",
    "            countShort += 1\n",
    "    print(\"... done.\")\n",
    "    print(\"Dict = \", countDict, \" (\", countWords, \" - \", countDict/countWords, \" ) and Short = \", countShort)\n",
    "    \n",
    "    return docs, corpus, idOriginal\n",
    "\n",
    "def addTags(docs):\n",
    "    dTags = []\n",
    "    analyzedDocument = namedtuple(\"AnalyzedDocument\", \"words tags\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        tags = [i]\n",
    "        dTags.append(analyzedDocument(doc, tags))\n",
    "    return dTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createDF():\n",
    "    df = pd.read_csv(\"data/listTweets.csv\")\n",
    "    docs, corpus, idOriginal = preProcess(df)\n",
    "    dTagsAll = addTags(docs)\n",
    "    print(\"Total nr. of documents = \", len(docs))\n",
    "    hashtags = df[df[\"ID\"].isin(idOriginal)][\"HASHTAG\"]\n",
    "    hashtags = [w.lower() for w in hashtags]\n",
    "        \n",
    "    dfProcessed = pd.DataFrame({\n",
    "    \"idOriginal\": idOriginal,\n",
    "    \"idCurrent\" : np.arange(len(docs)),\n",
    "    \"docs\"      : docs,\n",
    "    \"corpus\"    : corpus,\n",
    "    \"hashtag\"   : hashtags\n",
    "    })\n",
    "    \n",
    "    return dfProcessed\n",
    "\n",
    "def readDoc2VecModel(dm):\n",
    "    if dm==0:\n",
    "        return doc2vec.Doc2Vec.load(\"doc2vecTwitter.model\")\n",
    "    elif dm==1:\n",
    "        return doc2vec.Doc2Vec.load(\"doc2vecTwitter.model.dm\")\n",
    "    \n",
    "def prepareData(modelDoc2Vec, hashtags, nDocs):    \n",
    "    vecHashtags = []\n",
    "    for ht in hashtags:\n",
    "        vecHashtags.append(modelDoc2Vec.infer_vector(ht))\n",
    "    XAll = []\n",
    "    # construct XAll\n",
    "    for idx in range(nDocs):\n",
    "    #for idx in range(100):\n",
    "        tag = dfProcessed.idCurrent[idx]\n",
    "        xi = [i for i in modelDoc2Vec.docvecs[tag]]\n",
    "        for i in vecHashtags[idx]:\n",
    "            xi.append(i)\n",
    "        XAll.append(xi)\n",
    "    return XAll\n",
    "\n",
    "def prepareTraining(modelDoc2Vec, selected_column):\n",
    "    print(\"Preprocessing Training Data ...\")\n",
    "    dTrain1000 = pd.read_csv(\"data/sample_1000.csv\")\n",
    "    docsTraining, corpusTraining, idTraining = preProcess(dTrain1000)\n",
    "    dTags1000 = addTags(docsTraining)\n",
    "    hashtags = dTrain1000[dTrain1000[\"ID\"].isin(idTraining)][\"HASHTAG\"]\n",
    "    id2tag = {orig:curr for orig,curr in zip(dfProcessed.idOriginal, dfProcessed.idCurrent)}\n",
    "    vecHashtags = []\n",
    "    for ht in hashtags:\n",
    "        vecHashtags.append(modelDoc2Vec.infer_vector(ht))\n",
    "\n",
    "    Y = dTrain1000[dTrain1000[\"ID\"].isin(idTraining)][selected_column]\n",
    "    y = [y for y in Y]\n",
    "    tags = [id2tag[i] for i in idTraining]\n",
    "\n",
    "    X = []\n",
    "    #nn = 100\n",
    "    for idx in range(len(idTraining)):\n",
    "    #for idx in range(100):\n",
    "        tag = tags[idx]\n",
    "        if idx % 500 == 0:\n",
    "            print(\"Vector \", idx, \" with TAG = \", tag)\n",
    "        xi = [i for i in modelDoc2Vec.docvecs[tag]]\n",
    "        #print(xi)\n",
    "        for i in vecHashtags[idx]:\n",
    "            xi.append(i)\n",
    "        X.append(xi)\n",
    "    print(\"... done.\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# use general model to predict Y value of unseen data\n",
    "# note that dfProcessed here is a GLOBAL var\n",
    "# create general model to be used on unseen data\n",
    "def createModel(method, kk, X, y, dm):\n",
    "    if method == \"RF\":\n",
    "        print(\"** ** ** Using Method \", method, \" with dm = \", dm, \" ** ** **\")\n",
    "        mmAll = RandomForestClassifier(n_estimators=500, criterion=\"gini\", n_jobs=-1, max_features=\"sqrt\",class_weight={0:1, 1:100000})\n",
    "    elif method == \"SVM\":\n",
    "        print(\"** ** ** Using Method \", method, \" with dm = \", dm, \" [ kernel = \", kk, \"] ** ** **\")\n",
    "        mmAll = SVC(kernel=kk, class_weight={0: 1, 1: 100000}, gamma=0.0001, C=1E10)\n",
    "        #mmAll = SVC(kernel='linear', class_weight={0: 0.001, 1: 10000000}, gamma=0.01, C=1)\n",
    "    elif method == \"NN\":\n",
    "        print(\"** ** ** Using Method \", method, \" with dm = \", dm, \" ** ** **\")\n",
    "        mmAll = MLPClassifier(solver='lbfgs', alpha=0.5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "\n",
    "    mmAll.fit(X, np.array(y))\n",
    "    training_predictions = mmAll.predict(X)\n",
    "    #print(confusion_matrix(y, training_predictions))\n",
    "    #print(classification_report(y, training_predictions)) \n",
    "    \n",
    "    return mmAll\n",
    "\n",
    "def predictAll(mmAll, newCol, XAll):\n",
    "    training_predictions = mmAll.predict(XAll)\n",
    "    print(\"Nr. Ones = \", sum(training_predictions))\n",
    "    print(\"Done with prediction on all unseen data\")\n",
    "    dfProcessed[newCol] = training_predictions\n",
    "    \n",
    "#def getTraining(modelDoc2Vec, selected_column):\n",
    "#    df = pd.read_csv(\"data/sample_1000.csv\")\n",
    "#    idTrain = [int(i) for i in df[\"ID\"]]\n",
    "#    hashtags = dfProcessed[dfProcessed[\"idOriginal\"].isin(idTrain)][\"hashtag\"]\n",
    "#    ids = dfProcessed[dfProcessed[\"idOriginal\"].isin(idTrain)][\"idOriginal\"]\n",
    "    #y = df[df[\"ID\"].isin(idTrain)][\"COD_SE\"]\n",
    "#    y = df[df[\"ID\"].isin(idTrain)][selected_column]\n",
    "#    return hashtags, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length dictionary before filter =  2022361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-25 11:25:37,002 : INFO : discarding 1522361 tokens: [('al', 670883), ('alla', 533998), ('bagchee', 3), ('categoria', 695352), ('chandrakantha', 5), ('che', 694169), ('collegamenti', 623305), ('con', 695374), ('da', 678541), ('dal', 544442)]...\n",
      "2018-06-25 11:25:37,003 : INFO : keeping 500000 tokens which were in no less than 5 and no more than 524298 (=50.0%) documents\n",
      "2018-06-25 11:25:39,496 : INFO : resulting dictionary: Dictionary(500000 unique tokens: ['stadhuis', 'monétaire', 'köller', 'indecifrati', 'krunić']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length dictionary after filter =  500000\n",
      "Preprocessing Full Set of Documents ... \n",
      "Doc     0 has been processed\n",
      "Doc 10000 has been processed\n",
      "Doc 20000 has been processed\n",
      "... done.\n",
      "Dict =  83200  ( 329807  -  0.25226875111807817  ) and Short =  170\n",
      "Total nr. of documents =  23142\n"
     ]
    }
   ],
   "source": [
    "# preliminary libraries and stuff\n",
    "stop_words = stopwords.words(\"italian\")\n",
    "dct = Dictionary.load_from_text(\"/home/marco/gdrive/research/nlp/wiki/dictWiki.txt\")\n",
    "print(\"Length dictionary before filter = \", len(dct))\n",
    "dct.filter_extremes(keep_n=500000)\n",
    "print(\"Length dictionary after filter = \", len(dct))\n",
    "\n",
    "dfProcessed = createDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-25 11:26:19,261 : INFO : loading Doc2Vec object from doc2vecTwitter.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************************************************\n",
      "*\t \t \t \t COLUMN  COD_SE  \t \t \t \t*\n",
      "*********************************************************************************\n",
      "*********************************************************************************\n",
      "*\t \t \t \t CLASSIFICATION \t \t \t \t*\n",
      "*********************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-25 11:26:19,691 : INFO : loading docvecs recursively from doc2vecTwitter.model.docvecs.* with mmap=None\n",
      "2018-06-25 11:26:19,692 : INFO : loading wv recursively from doc2vecTwitter.model.wv.* with mmap=None\n",
      "2018-06-25 11:26:19,693 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-06-25 11:26:19,695 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-25 11:26:19,696 : INFO : loaded doc2vecTwitter.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Training Data ...\n",
      "Preprocessing Full Set of Documents ... \n",
      "Doc     0 has been processed\n",
      "... done.\n",
      "Dict =  3530  ( 14254  -  0.24764978251718817  ) and Short =  7\n",
      "Vector  0  with TAG =  19\n",
      "Vector  500  with TAG =  11848\n",
      "... done.\n",
      "** ** ** Using Method  RF  with dm =  0  ** ** **\n",
      "Nr. Ones =  138\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  0  [ kernel =  sigmoid ] ** ** **\n",
      "Nr. Ones =  3943\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  0  [ kernel =  linear ] ** ** **\n",
      "Nr. Ones =  3942\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  0  [ kernel =  poly ] ** ** **\n",
      "Nr. Ones =  1697\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  0  [ kernel =  rbf ] ** ** **\n",
      "Nr. Ones =  3762\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  NN  with dm =  0  ** ** **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-25 11:27:08,417 : INFO : loading Doc2Vec object from doc2vecTwitter.model.dm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. Ones =  1809\n",
      "Done with prediction on all unseen data\n",
      "*********************************************************************************\n",
      "*\t \t \t \t CLASSIFICATION \t \t \t \t*\n",
      "*********************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-25 11:27:08,943 : INFO : loading docvecs recursively from doc2vecTwitter.model.dm.docvecs.* with mmap=None\n",
      "2018-06-25 11:27:08,945 : INFO : loading wv recursively from doc2vecTwitter.model.dm.wv.* with mmap=None\n",
      "2018-06-25 11:27:08,946 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-06-25 11:27:08,948 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-25 11:27:08,949 : INFO : loaded doc2vecTwitter.model.dm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Training Data ...\n",
      "Preprocessing Full Set of Documents ... \n",
      "Doc     0 has been processed\n",
      "... done.\n",
      "Dict =  3530  ( 14254  -  0.24764978251718817  ) and Short =  7\n",
      "Vector  0  with TAG =  19\n",
      "Vector  500  with TAG =  11848\n",
      "... done.\n",
      "** ** ** Using Method  RF  with dm =  1  ** ** **\n",
      "Nr. Ones =  154\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  1  [ kernel =  sigmoid ] ** ** **\n",
      "Nr. Ones =  4177\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  1  [ kernel =  linear ] ** ** **\n",
      "Nr. Ones =  4174\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  1  [ kernel =  poly ] ** ** **\n",
      "Nr. Ones =  6469\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  1  [ kernel =  rbf ] ** ** **\n",
      "Nr. Ones =  3539\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  NN  with dm =  1  ** ** **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-25 11:28:18,351 : INFO : loading Doc2Vec object from doc2vecTwitter.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. Ones =  1800\n",
      "Done with prediction on all unseen data\n",
      "*********************************************************************************\n",
      "*\t \t \t \t COLUMN  COD_ME  \t \t \t \t*\n",
      "*********************************************************************************\n",
      "*********************************************************************************\n",
      "*\t \t \t \t CLASSIFICATION \t \t \t \t*\n",
      "*********************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-25 11:28:18,864 : INFO : loading docvecs recursively from doc2vecTwitter.model.docvecs.* with mmap=None\n",
      "2018-06-25 11:28:18,866 : INFO : loading wv recursively from doc2vecTwitter.model.wv.* with mmap=None\n",
      "2018-06-25 11:28:18,867 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-06-25 11:28:18,868 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-25 11:28:18,870 : INFO : loaded doc2vecTwitter.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Training Data ...\n",
      "Preprocessing Full Set of Documents ... \n",
      "Doc     0 has been processed\n",
      "... done.\n",
      "Dict =  3530  ( 14254  -  0.24764978251718817  ) and Short =  7\n",
      "Vector  0  with TAG =  19\n",
      "Vector  500  with TAG =  11848\n",
      "... done.\n",
      "** ** ** Using Method  RF  with dm =  0  ** ** **\n",
      "Nr. Ones =  36\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  0  [ kernel =  sigmoid ] ** ** **\n",
      "Nr. Ones =  1973\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  0  [ kernel =  linear ] ** ** **\n",
      "Nr. Ones =  1973\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  0  [ kernel =  poly ] ** ** **\n",
      "Nr. Ones =  516\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  0  [ kernel =  rbf ] ** ** **\n",
      "Nr. Ones =  1925\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  NN  with dm =  0  ** ** **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-25 11:28:55,344 : INFO : loading Doc2Vec object from doc2vecTwitter.model.dm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. Ones =  1003\n",
      "Done with prediction on all unseen data\n",
      "*********************************************************************************\n",
      "*\t \t \t \t CLASSIFICATION \t \t \t \t*\n",
      "*********************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-25 11:28:55,747 : INFO : loading docvecs recursively from doc2vecTwitter.model.dm.docvecs.* with mmap=None\n",
      "2018-06-25 11:28:55,748 : INFO : loading wv recursively from doc2vecTwitter.model.dm.wv.* with mmap=None\n",
      "2018-06-25 11:28:55,749 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-06-25 11:28:55,750 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-25 11:28:55,752 : INFO : loaded doc2vecTwitter.model.dm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Training Data ...\n",
      "Preprocessing Full Set of Documents ... \n",
      "Doc     0 has been processed\n",
      "... done.\n",
      "Dict =  3530  ( 14254  -  0.24764978251718817  ) and Short =  7\n",
      "Vector  0  with TAG =  19\n",
      "Vector  500  with TAG =  11848\n",
      "... done.\n",
      "** ** ** Using Method  RF  with dm =  1  ** ** **\n",
      "Nr. Ones =  44\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  1  [ kernel =  sigmoid ] ** ** **\n",
      "Nr. Ones =  2702\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  1  [ kernel =  linear ] ** ** **\n",
      "Nr. Ones =  2702\n",
      "Done with prediction on all unseen data\n",
      "** ** ** Using Method  SVM  with dm =  1  [ kernel =  poly ] ** ** **\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-65a5e18887d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"SVM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0mmmAll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                     \u001b[0mnewCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_dm\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mpredictAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXAll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-117b816d0a72>\u001b[0m in \u001b[0;36mcreateModel\u001b[0;34m(method, kk, X, y, dm)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmmAll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtraining_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmAll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(confusion_matrix(y, training_predictions))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marco/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marco/anaconda3/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cycle over different methods and kernels\n",
    "cols = [\"SE\", \"ME\"]  #two types of emotions\n",
    "dm = [0,1]\n",
    "methods = [\"RF\", \"SVM\", \"NN\"]\n",
    "kernels = [\"sigmoid\", \"linear\", \"poly\", \"rbf\"]\n",
    "for cc in cols:\n",
    "    selected_column = \"COD_\" + cc\n",
    "    print(\"*\"*81)\n",
    "    print(\"*\\t \\t \\t \\t COLUMN \", selected_column, \" \\t \\t \\t \\t*\")\n",
    "    print(\"*\"*81)\n",
    "\n",
    "    for dd in dm:\n",
    "        print(\"*\"*81)\n",
    "        print(\"*\\t \\t \\t \\t CLASSIFICATION \\t \\t \\t \\t*\")\n",
    "        print(\"*\"*81)\n",
    "\n",
    "        modelDoc2Vec = readDoc2VecModel(dd)\n",
    "        #hashtags = dfProcessed[\"hashtag\"]\n",
    "        XAll = prepareData(modelDoc2Vec, dfProcessed[\"hashtag\"], len(dfProcessed))\n",
    "        X, y = prepareTraining(modelDoc2Vec, selected_column)\n",
    "\n",
    "        for meth in methods:\n",
    "            if meth == \"SVM\":\n",
    "                for kk in kernels:\n",
    "                    mmAll = createModel(meth, kk, X, y, dd)\n",
    "                    newCol = cc + \"_dm\" + str(dd) + \"_\" + str(meth) + \"_\" + str(kk)\n",
    "                    predictAll(mmAll, newCol, XAll)\n",
    "            elif meth == \"RF\":\n",
    "                mmAll = createModel(meth, \"\", X, y, dd)\n",
    "                newCol = cc + \"_dm\" + str(dd) + \"_\" + str(meth)\n",
    "                predictAll(mmAll, newCol, XAll)\n",
    "            elif meth == \"NN\":\n",
    "                mmAll = createModel(meth, \"\", X, y, dd)\n",
    "                newCol = cc + \"_dm\" + str(dd) + \"_\" + str(meth)\n",
    "                predictAll(mmAll, newCol, XAll)\n",
    "\n",
    "print(\"Classification saved on disk...\")\n",
    "dfProcessed.to_csv(\"data/classifyUnseen.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
